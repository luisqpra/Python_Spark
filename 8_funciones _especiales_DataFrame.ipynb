{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Fehca y hora\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import datediff, months_between, last_day\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "from pyspark.sql.functions import (year, month, dayofmonth,\n",
    "                                   dayofyear, hour, minute, second)\n",
    "\n",
    "# Cadenas de texto\n",
    "from pyspark.sql.functions import ltrim, rtrim, trim\n",
    "from pyspark.sql.functions import col, lpad, rpad\n",
    "from pyspark.sql.functions import concat_ws, lower, upper, initcap, reverse\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Colecciones\n",
    "from pyspark.sql.functions import col, size, sort_array, array_contains\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "from pyspark.sql.functions import from_json, to_json\n",
    "\n",
    "from pyspark.sql.functions import col, when, lit, coalesce\n",
    "\n",
    "# Funciones predefinidas por el usuario\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Funciones de ventana\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, row_number, rank, dense_rank, col\n",
    "from pyspark.sql.functions import min, max, avg\n",
    "\n",
    "# Catalyst Optimizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "path='files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/09 09:56:45 WARN Utils: Your hostname, luis-Nitro-AN515-52 resolves to a loopback address: 127.0.1.1; using 192.168.1.17 instead (on interface enp7s0f1)\n",
      "24/01/09 09:56:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/09 09:56:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fecha y Hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- ts_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet(path+'convertir')\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+----------------+\n",
      "|      date|           timestamp|  date_str|          ts_str|\n",
      "+----------+--------------------+----------+----------------+\n",
      "|2021-01-01|2021-01-01 20:10:...|01-01-2021|18-08-2021 46:58|\n",
      "+----------+--------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### date & timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+-------------------+\n",
      "|date1     |ts1                    |date2     |ts2                |\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|2021-01-01|2021-08-18 00:46:58|\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data1 = data.select(\n",
    "    to_date(col('date')).alias('date1'),\n",
    "    to_timestamp(col('timestamp')).alias('ts1'),\n",
    "    to_date(col('date_str'), 'dd-MM-yyyy').alias('date2'),\n",
    "    to_timestamp(col('ts_str'), 'dd-MM-yyyy mm:ss').alias('ts2')\n",
    "\n",
    ")\n",
    "\n",
    "data1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date1: date (nullable = true)\n",
      " |-- ts1: timestamp (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- ts2: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|date_format(date1, dd-MM-yyyy)|\n",
      "+------------------------------+\n",
      "|                    01-01-2021|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dar formato de fecha\n",
    "data1.select(\n",
    "    date_format(col('date1'), 'dd-MM-yyyy')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+-------------------+\n",
      "|nombre|fecha_ingreso|fecha_salida|       baja_sistema|\n",
      "+------+-------------+------------+-------------------+\n",
      "|  Jose|   2021-01-01|  2021-11-14|2021-10-14 15:35:59|\n",
      "|Mayara|   2021-02-06|  2021-11-25|2021-11-25 10:35:55|\n",
      "+------+-------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calculo = spark.read.parquet(path+'calculo')\n",
    "calculo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### datediff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+--------------+\n",
      "|nombre|dias|      meses|ultimo_dia_mes|\n",
      "+------+----+-----------+--------------+\n",
      "|  Jose| 317|10.41935484|    2021-11-30|\n",
      "|Mayara| 292| 9.61290323|    2021-11-30|\n",
      "+------+----+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculos de fecha y hora\n",
    "calculo.select(\n",
    "    col('nombre'),\n",
    "    datediff(col('fecha_salida'), col('fecha_ingreso')).alias('dias'),\n",
    "    months_between(col('fecha_salida'), col('fecha_ingreso')).alias('meses'),\n",
    "    last_day(col('fecha_salida')).alias('ultimo_dia_mes')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add & sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+-----------+\n",
      "|nombre|fecha_ingreso|mas_14_dias|menos_1_dia|\n",
      "+------+-------------+-----------+-----------+\n",
      "|  Jose|   2021-01-01| 2021-01-15| 2020-12-31|\n",
      "|Mayara|   2021-02-06| 2021-02-20| 2021-02-05|\n",
      "+------+-------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sumar y restar fechas\n",
    "calculo.select(\n",
    "    col('nombre'),\n",
    "    col('fecha_ingreso'),\n",
    "    date_add(col('fecha_ingreso'), 14).alias('mas_14_dias'),\n",
    "    date_sub(col('fecha_ingreso'), 1).alias('menos_1_dia')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### year, month, day, hour, minute, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|       baja_sistema|year(baja_sistema)|month(baja_sistema)|dayofmonth(baja_sistema)|dayofyear(baja_sistema)|hour(baja_sistema)|minute(baja_sistema)|second(baja_sistema)|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|2021-10-14 15:35:59|              2021|                 10|                      14|                    287|                15|                  35|                  59|\n",
      "|2021-11-25 10:35:55|              2021|                 11|                      25|                    329|                10|                  35|                  55|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracion de datos de la fecha\n",
    "calculo.select(\n",
    "    col('baja_sistema'),\n",
    "    year(col('baja_sistema')),\n",
    "    month(col('baja_sistema')),\n",
    "    dayofmonth(col('baja_sistema')),\n",
    "    dayofyear(col('baja_sistema')),\n",
    "    hour(col('baja_sistema')),\n",
    "    minute(col('baja_sistema')),\n",
    "    second(col('baja_sistema'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadenas de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| nombre|\n",
      "+-------+\n",
      "| Spark |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_str = spark.read.parquet(path+'data1.parquet')\n",
    "data_str.show() # ' Spark '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "| ltrim| rtrim| trim|\n",
      "+------+------+-----+\n",
      "|Spark | Spark|Spark|\n",
      "+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Borrar espacios\n",
    "data_str.select(\n",
    "    ltrim('nombre').alias('ltrim'), # Borrar espacios a la izquierda\n",
    "    rtrim('nombre').alias('rtrim'), # Borrar espacios a la derecha\n",
    "    trim('nombre').alias('trim') # Borrar espacios a la derecha e izquierda\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      lpad|      rpad|\n",
      "+----------+----------+\n",
      "|-*-*-Spark|Spark^=^=^|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregar caracteres \n",
    "data_str.select(\n",
    "    trim(col('nombre')).alias('trim') # Borrar espacios \n",
    ").select(\n",
    "    lpad(col('trim'), 10, '-*').alias('lpad'), # agregar caracteres a la izquierda\n",
    "    rpad(col('trim'), 10, '^=').alias('rpad') # agregar caracteres a la derecha\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+\n",
      "|sujeto|verbo|   adjetivo|\n",
      "+------+-----+-----------+\n",
      "| Spark|   es|maravilloso|\n",
      "+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [('Spark', 'es', 'maravilloso')],\n",
    "    ['sujeto', 'verbo', 'adjetivo'])\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               frase|           minuscula|           mayuscula|             initcap|             reversa|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Spark es maravilloso|spark es maravilloso|SPARK ES MARAVILLOSO|Spark Es Maravilloso|osollivaram se krapS|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformaciones de texto\n",
    "df1.select(\n",
    "    concat_ws(' ', col('sujeto'), col('verbo'), col('adjetivo')).alias('frase') # unir strr\n",
    ").select(\n",
    "    col('frase'),\n",
    "    lower(col('frase')).alias('minuscula'), # minuscula\n",
    "    upper(col('frase')).alias('mayuscula'), # mayuscula\n",
    "    initcap(col('frase')).alias('initcap'), # primera letra de cada palabra en mayuscula\n",
    "    reverse(col('frase')).alias('reversa') # invertir texto\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|frase                     |\n",
      "+--------------------------+\n",
      "| voy a casa por mis llaves|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([(' voy a casa por mis llaves',)], ['frase'])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|nueva_frase             |\n",
      "+------------------------+\n",
      "| ir a casa ir mis llaves|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aplicar expresiones regulares\n",
    "df2.select(\n",
    "    regexp_replace(col('frase'), 'voy|por', 'ir').alias('nueva_frase')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------+\n",
      "|dia  |tareas                                      |\n",
      "+-----+--------------------------------------------+\n",
      "|lines|[hacer la tarea, buscar agua, lavar el auto]|\n",
      "+-----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_cole= spark.createDataFrame([('lines', ['hacer la tarea', 'buscar agua', 'lavar el auto'])], ['dia','tareas'])\n",
    "data_cole.show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------+-----------+\n",
      "|tamaño|arreglo_ordenado                            |buscar_agua|\n",
      "+------+--------------------------------------------+-----------+\n",
      "|3     |[buscar agua, hacer la tarea, lavar el auto]|true       |\n",
      "+------+--------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_cole.select(\n",
    "    size(col('tareas')).alias('tamaño'), # conteo de elementos\n",
    "    sort_array(col('tareas')).alias('arreglo_ordenado'), # organizar los elementos\n",
    "    array_contains(col('tareas'), 'buscar agua').alias('buscar_agua') # busqueda de elementos\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|  dia|        tareas|\n",
      "+-----+--------------+\n",
      "|lines|hacer la tarea|\n",
      "|lines|   buscar agua|\n",
      "|lines| lavar el auto|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_cole.select(\n",
    "    col('dia'),\n",
    "    explode(col('tareas')).alias('tareas') # extraer cada elemento\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------+\n",
      "|dia  |tareas                                      |\n",
      "+-----+--------------------------------------------+\n",
      "|lunes|[hacer la tarea, buscar agua, lavar el auto]|\n",
      "+-----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_json= spark.createDataFrame([({\"dia\":\"lunes\",\"tareas\":['hacer la tarea', 'buscar agua', 'lavar el auto']})])\n",
    "data_json.show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When, coalesce y lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|nombre|pago|\n",
      "+------+----+\n",
      "|  Jose|   1|\n",
      "| Julia|   2|\n",
      "| Katia|   1|\n",
      "|  NULL|   3|\n",
      "|  Raul|   3|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet(path+'data2.parquet')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|nombre|       pago|\n",
      "+------+-----------+\n",
      "|  Jose|     pagado|\n",
      "| Julia|  sin pagar|\n",
      "| Katia|     pagado|\n",
      "|  NULL|sin iniciar|\n",
      "|  Raul|sin iniciar|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when funciona como una funcion switch\n",
    "data.select(\n",
    "    col('nombre'),\n",
    "    when(col('pago') == 1, 'pagado') # opcion 1\n",
    "    .when(col('pago') == 2, 'sin pagar') # opcion 2\n",
    "    .otherwise('sin iniciar').alias('pago') # opcion default\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    nombre|\n",
      "+----------+\n",
      "|      Jose|\n",
      "|     Julia|\n",
      "|     Katia|\n",
      "|sin nombre|\n",
      "|      Raul|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    coalesce(col('nombre'), lit('sin nombre')).alias('nombre') # reemplazar datos vacios NULL\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones definidas por el usuario UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion cubo\n",
    "def f_cubo(n):\n",
    "    return n * n * n\n",
    "\n",
    "# Funcion bienvenida\n",
    "def bienvenida(nombre):\n",
    "    return ('Hola {}'.format(nombre))\n",
    "\n",
    "# Funcion mayuscula\n",
    "@udf(returnType=StringType())\n",
    "def mayuscula(s):\n",
    "    return s.upper()\n",
    "\n",
    "# Funcion cubo con pandas\n",
    "def cubo_pandas(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion cubo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/09 10:09:48 WARN SimpleFunctionRegistry: The function cubo replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# crear la funcion en spark\n",
    "spark.udf.register('cubo', f_cubo, LongType()) # agregar a sesiom de spark\n",
    "bienvenida_udf = udf(lambda x: bienvenida(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vista temporal\n",
    "spark.range(1, 10).createOrReplaceTempView('df_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|cubo|\n",
      "+---+----+\n",
      "|  1|   1|\n",
      "|  2|   8|\n",
      "|  3|  27|\n",
      "|  4|  64|\n",
      "|  5| 125|\n",
      "|  6| 216|\n",
      "|  7| 343|\n",
      "|  8| 512|\n",
      "|  9| 729|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# consulta sql\n",
    "spark.sql(\"SELECT id, cubo(id) AS cubo FROM df_temp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion bienvenida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|nombre|\n",
      "+------+\n",
      "|  Jose|\n",
      "| Julia|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nombre = spark.createDataFrame([('Jose',), ('Julia',)], ['nombre'])\n",
    "df_nombre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|nombre|bie_nombre|\n",
      "+------+----------+\n",
      "|  Jose| Hola Jose|\n",
      "| Julia|Hola Julia|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplicar funcion bienvenida_udf\n",
    "df_nombre.select(\n",
    "    col('nombre'),\n",
    "    bienvenida_udf(col('nombre')).alias('bie_nombre')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion Mayuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|nombre|may_nombre|\n",
      "+------+----------+\n",
      "|  Jose|      JOSE|\n",
      "| Julia|     JULIA|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nombre.select(\n",
    "    col('nombre'),\n",
    "    mayuscula(col('nombre')).alias('may_nombre')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion cubo con pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubo_udf = pandas_udf(cubo_pandas, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|cubo_pandas|\n",
      "+---+-----------+\n",
      "|  0|          0|\n",
      "|  1|          1|\n",
      "|  2|          8|\n",
      "|  3|         27|\n",
      "|  4|         64|\n",
      "+---+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.range(5)\n",
    "\n",
    "df.select(\n",
    "    col('id'),\n",
    "    cubo_udf(col('id')).alias('cubo_pandas')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion ventanas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+\n",
      "| nombre|edad|departamento|evaluacion|\n",
      "+-------+----+------------+----------+\n",
      "| Lazaro|  45|      letras|        98|\n",
      "|   Raul|  24|  matemática|        76|\n",
      "|  Maria|  34|  matemática|        27|\n",
      "|   Jose|  30|     química|        78|\n",
      "| Susana|  51|     química|        98|\n",
      "|   Juan|  44|      letras|        89|\n",
      "|  Julia|  55|      letras|        92|\n",
      "|  Kadir|  38|arquitectura|        39|\n",
      "| Lilian|  23|arquitectura|        94|\n",
      "|   Rosa|  26|      letras|        91|\n",
      "|   Aian|  50|  matemática|        73|\n",
      "|Yaneisy|  29|      letras|        89|\n",
      "|Enrique|  40|     química|        92|\n",
      "|    Jon|  25|arquitectura|        78|\n",
      "|  Luisa|  39|arquitectura|        94|\n",
      "+-------+----+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ventana = spark.read.parquet(path+'funciones_ventana.parquet')\n",
    "df_ventana.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear subgrupos (ventanas) del dataframe\n",
    "\n",
    "windowSpec = Window.partitionBy('departamento').orderBy(desc('evaluacion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "| nombre|edad|departamento|evaluacion|row_number|\n",
      "+-------+----+------------+----------+----------+\n",
      "| Lilian|  23|arquitectura|        94|         1|\n",
      "|  Luisa|  39|arquitectura|        94|         2|\n",
      "|    Jon|  25|arquitectura|        78|         3|\n",
      "|  Kadir|  38|arquitectura|        39|         4|\n",
      "| Lazaro|  45|      letras|        98|         1|\n",
      "|  Julia|  55|      letras|        92|         2|\n",
      "|   Rosa|  26|      letras|        91|         3|\n",
      "|   Juan|  44|      letras|        89|         4|\n",
      "|Yaneisy|  29|      letras|        89|         5|\n",
      "|   Raul|  24|  matemática|        76|         1|\n",
      "|   Aian|  50|  matemática|        73|         2|\n",
      "|  Maria|  34|  matemática|        27|         3|\n",
      "| Susana|  51|     química|        98|         1|\n",
      "|Enrique|  40|     química|        92|         2|\n",
      "|   Jose|  30|     química|        78|         3|\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# row_number\n",
    "# numero de fila por ventana\n",
    "df_ventana.withColumn('row_number', row_number().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----+\n",
      "| nombre|edad|departamento|evaluacion|rank|\n",
      "+-------+----+------------+----------+----+\n",
      "| Lilian|  23|arquitectura|        94|   1|\n",
      "|  Luisa|  39|arquitectura|        94|   1|\n",
      "|    Jon|  25|arquitectura|        78|   3|\n",
      "|  Kadir|  38|arquitectura|        39|   4|\n",
      "| Lazaro|  45|      letras|        98|   1|\n",
      "|  Julia|  55|      letras|        92|   2|\n",
      "|   Rosa|  26|      letras|        91|   3|\n",
      "|   Juan|  44|      letras|        89|   4|\n",
      "|Yaneisy|  29|      letras|        89|   4|\n",
      "|   Raul|  24|  matemática|        76|   1|\n",
      "|   Aian|  50|  matemática|        73|   2|\n",
      "|  Maria|  34|  matemática|        27|   3|\n",
      "| Susana|  51|     química|        98|   1|\n",
      "|Enrique|  40|     química|        92|   2|\n",
      "|   Jose|  30|     química|        78|   3|\n",
      "+-------+----+------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rank\n",
    "# ranking\n",
    "df_ventana.withColumn('rank', rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "| nombre|edad|departamento|evaluacion|dense_rank|\n",
      "+-------+----+------------+----------+----------+\n",
      "| Lilian|  23|arquitectura|        94|         1|\n",
      "|  Luisa|  39|arquitectura|        94|         1|\n",
      "|    Jon|  25|arquitectura|        78|         2|\n",
      "|  Kadir|  38|arquitectura|        39|         3|\n",
      "| Lazaro|  45|      letras|        98|         1|\n",
      "|  Julia|  55|      letras|        92|         2|\n",
      "|   Rosa|  26|      letras|        91|         3|\n",
      "|   Juan|  44|      letras|        89|         4|\n",
      "|Yaneisy|  29|      letras|        89|         4|\n",
      "|   Raul|  24|  matemática|        76|         1|\n",
      "|   Aian|  50|  matemática|        73|         2|\n",
      "|  Maria|  34|  matemática|        27|         3|\n",
      "| Susana|  51|     química|        98|         1|\n",
      "|Enrique|  40|     química|        92|         2|\n",
      "|   Jose|  30|     química|        78|         3|\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dense_rank\n",
    "\n",
    "df_ventana.withColumn('dense_rank', dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+---+---+------------------+----------+----+----------+\n",
      "| nombre|edad|departamento|evaluacion|min|max|               avg|row_number|rank|dense_rank|\n",
      "+-------+----+------------+----------+---+---+------------------+----------+----+----------+\n",
      "| Lilian|  23|arquitectura|        94| 39| 94|             76.25|         1|   1|         1|\n",
      "|  Luisa|  39|arquitectura|        94| 39| 94|             76.25|         2|   1|         1|\n",
      "|    Jon|  25|arquitectura|        78| 39| 94|             76.25|         3|   3|         2|\n",
      "|  Kadir|  38|arquitectura|        39| 39| 94|             76.25|         4|   4|         3|\n",
      "| Lazaro|  45|      letras|        98| 89| 98|              91.8|         1|   1|         1|\n",
      "|  Julia|  55|      letras|        92| 89| 98|              91.8|         2|   2|         2|\n",
      "|   Rosa|  26|      letras|        91| 89| 98|              91.8|         3|   3|         3|\n",
      "|   Juan|  44|      letras|        89| 89| 98|              91.8|         4|   4|         4|\n",
      "|Yaneisy|  29|      letras|        89| 89| 98|              91.8|         5|   4|         4|\n",
      "|   Raul|  24|  matemática|        76| 27| 76|58.666666666666664|         1|   1|         1|\n",
      "|   Aian|  50|  matemática|        73| 27| 76|58.666666666666664|         2|   2|         2|\n",
      "|  Maria|  34|  matemática|        27| 27| 76|58.666666666666664|         3|   3|         3|\n",
      "| Susana|  51|     química|        98| 78| 98| 89.33333333333333|         1|   1|         1|\n",
      "|Enrique|  40|     química|        92| 78| 98| 89.33333333333333|         2|   2|         2|\n",
      "|   Jose|  30|     química|        78| 78| 98| 89.33333333333333|         3|   3|         3|\n",
      "+-------+----+------------+----------+---+---+------------------+----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregaciones con especificaciones de ventana\n",
    "\n",
    "windowSpecAgg = Window.partitionBy('departamento')\n",
    "\n",
    "(df_ventana.withColumn('min', min(col('evaluacion')).over(windowSpecAgg))\n",
    "    .withColumn('max', max(col('evaluacion')).over(windowSpecAgg))\n",
    "    .withColumn('avg', avg(col('evaluacion')).over(windowSpecAgg))\n",
    "    .withColumn('row_number', row_number().over(windowSpec))\n",
    "    .withColumn('rank', rank().over(windowSpec))\n",
    "    .withColumn('dense_rank', dense_rank().over(windowSpec))\n",
    " ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
